---
title: "JY Pathology scores"
author: "Clemens Hug"
output:
  html_document:
    theme: flatly
    toc: yes
    number_sections: true
    toc_float:
      collapsed: false
    toc_depth: 4
    code_folding: hide
---

```{r setup}
library(tidyverse)
library(DT)
library(ordinal)
library(broom.mixed)
library(coin)
library(DescTools)
library(googlesheets4)
library(powerjoin)
library(brms)
library(janitor)
library(qs)
library(ggbeeswarm)
library(ggpubr)
library(tidybayes)
library(here)

theme_set(
  theme_minimal(base_family = "Helvetica")
)
```

## Loading data

```{r}
patho_scores_raw <- read_sheet(
  "https://docs.google.com/spreadsheets/d/1eATs2B_xOC9rx1VN9dDIbHpNp9iPk6Q7hrkl8zfP9Dk/edit?gid=0#gid=0"
) %>%
  drop_na(`LSP ID`) %>%
  rename(
    Albumin = `Albumin (tubule)`
  )

# clinical_scores_raw <- read_sheet(
#   "https://docs.google.com/spreadsheets/d/1eATs2B_xOC9rx1VN9dDIbHpNp9iPk6Q7hrkl8zfP9Dk/edit?gid=0#gid=0",
#   sheet = 2
# ) %>%
#   drop_na(`LSP ID`) %>%
#   rename(
#     C3 = `C3 Glom`
#   )

orion_quants_and_clinical_raw <- read_sheet(
  "https://docs.google.com/spreadsheets/d/1a1nLyFQNTJYoVWLX9JS2wIItvTqnxLVJ30NXdnLd7AE/edit?gid=382231029#gid=382231029",
  sheet = 1
)

clinical_scores_raw <- orion_quants_and_clinical_raw %>%
  select(
    `LSP ID` = slideName,
    ends_with("_score")
  ) %>%
  rename_with(\(x) str_remove(x, fixed("_score"))) %>%
  rename(
    `Lambda LC` = Lambda,
    `Kappa LC` = Kappa
  )

orion_quants_raw <- orion_quants_and_clinical_raw %>%
  select(
    `LSP ID` = slideName,
    starts_with("mean_")
  ) %>%
  rename_with(\(x) str_remove(x, fixed("mean_"))) %>%
  rename(
    `Lambda LC` = Lambda,
    `Kappa LC` = Kappa
  )
```

```{r dt-setup}
dt_options <- list(
  dom = "Bfrtip",
  buttons = "copy"
)

dt_format_signif <- function(dt) {
  formatSignif(
    dt,
    keep(dt$x$data, is.numeric) %>% names(),
    digits = 2
  )
}

dt_default_table <- function(df) {
  datatable(
    df,
    extensions = "Buttons",
    options = dt_options
  ) %>%
    dt_format_signif()
}
```


## Data processing

```{r}
markers <- reduce(
  list(
    colnames(patho_scores_raw),
    colnames(clinical_scores_raw),
    colnames(orion_quants_raw)
  ),
  intersect
) %>%
  setdiff(c("LSP ID"))

score_order <- c("0", "0.5", "1", "2", "3", "4")
score_mapping <- c(
  `0.25` = "0.5",
  `0.75` = "1",
  `1.5` = "2"
)

scores_all <- bind_rows(
  pathologist = patho_scores_raw %>%
    transmute(
      `LSP ID`,
      across(
        all_of(markers),
        \(x) as.character(str_remove(x, fixed("+")))
      )
    ),
  clinical = clinical_scores_raw %>%
    transmute(
      `LSP ID`,
      across(
        all_of(markers),
        \(x) as.character(x) %>%
          recode(
            !!!score_mapping
          )
      )
    ),
  .id = "source"
) %>%
  pivot_longer(
    -c(`LSP ID`, source),
    names_to = "marker",
    values_to = "score"
  ) %>%
  mutate(
    across(score, \(x) factor(x, levels = score_order, ordered = TRUE))
  ) %>%
  group_by(`LSP ID`, marker) %>%
  mutate(
    both_sources = n_distinct(source) == 2
  ) %>%
  ungroup()

orion_quants <- orion_quants_raw %>%
  pivot_longer(
    -`LSP ID`,
    names_to = "marker",
    values_to = "expression"
  ) %>%
  mutate(
    log_expression = log10(expression)
  )
```

Checking `LSP ID` overlap between the datasets

```{r}
setdiff(scores_all$`LSP ID`, orion_quants$`LSP ID`)
setdiff(orion_quants$`LSP ID`, scores_all$`LSP ID`)
```

All `LSP ID`s are present in both datasets as expected.

## Confusion matrices

```{r}
scores_confusion <- scores_all %>%
  filter(both_sources) %>%
  select(-both_sources) %>%
  pivot_wider(
    names_from = source,
    values_from = score
  ) %>%
  count(marker, pathologist, clinical)

p <- scores_confusion %>%
  complete(
    marker, pathologist, clinical,
    fill = list(n = 0)
  ) %>%
  ggplot(aes(x = pathologist, y = clinical, fill = n)) +
  geom_tile() +
  geom_text(
    aes(label = n),
    color = "white"
  ) +
  scale_fill_continuous(
    trans = scales::transform_pseudo_log()
  ) +
  facet_wrap(~marker) +
  coord_equal()

p
```

## Cohen’s Kappa Statistic

Purpose: Measures the agreement between two raters
(in this case, the pathologist and the algorithm) beyond what would be expected by chance.

It is computed based on the confusion tables above.

Interpretation: Values range from -1 to 1, where:

* < 0: Less agreement than expected by chance
* 0.01–0.20: Slight agreement
* 0.21–0.40: Fair agreement
* 0.41–0.60: Moderate agreement
* 0.61–0.80: Substantial agreement
* 0.81–1.00: Almost perfect agreement

Source for score ranges: https://doi.org/10.2307%2F2529310

```{r}
mods <- scores_all %>%
  filter(both_sources) %>%
  select(-both_sources) %>%
  group_nest(marker) %>%
  rowwise() %>%
  mutate(
    mod = list(
      clmm(
        score ~ source + (1 | `LSP ID`),
        data = data,
        link = "logit"
      )
    ),
    kappa = list(
      KappaM(
        data %>%
          pivot_wider(
            names_from = source,
            values_from = score
          ) %>%
          select(-`LSP ID`),
        conf.level = .95,
        method = "Conger"
      )
    ),
    kappa_weighted = list(
      irr::kappa2(
        data %>%
          pivot_wider(
            names_from = source,
            values_from = score
          ) %>%
          select(-`LSP ID`),
        weight = "squared"
      )
    )
  ) %>%
  ungroup()

kappa_df <- mods %>%
  transmute(
    marker,
    kappa = map(kappa, \(x) tibble(kappa = x[["kappa"]])),
    kappa_weighted = map(
      kappa_weighted,
      \(x) as.data.frame(magrittr::set_class(x, "list")) %>%
        select(kappa = value, p.value)
    )
  ) %>%
  pivot_longer(starts_with("kappa"), names_to = "kappa_type", values_to = "kappa") %>%
  unnest(kappa)
```

### Kappa table

```{r}
dt_default_table(
  kappa_df
)
```

The weighted kappa allows disagreements to be weighted differently according to
the distance between the scores. This is useful when the scores are ordered.

It is I would say the correct kappa metric to use here, since the scores are ordered,
not categorical, so the distances are meaningful.

`p.value` here tells you if the kappa is significantly higher than 0, meaning
a significant value tells you that the agreement is higher than expected by chance.

```{r}
p <- kappa_df %>%
  ggplot(aes(x = marker, y = kappa, fill = kappa_type)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_col(position = "dodge") +
  facet_wrap(~kappa_type, ncol = 1)

p
```


## Ordinal mixed model

While Kappa gives you a measurement for agreement, it doesn't tell you what the
differences are when there is disagreement. We can use an ordinal mixed model
to tell us that. It basically can tell us if the pathologist scored higher or
lower than the clinical report.

Only use 53 cases that have clinical and pathologist scores

```{r}
mods_df <- mods %>%
  transmute(
    marker,
    res = map(mod, tidy)
  ) %>%
  unnest(res) %>%
  filter(term =="sourcepathologist") %>%
  select(
    marker, estimate, std.error, p.value
  )
```

### Ordinal mixed model table

Here we are testing for differences in mean scores between clinical reports
and your pathologist using an ordinal mixed model. `estimate` is a slope showing
the magnitude of difference in scores between the pathologist and the clinical.
0 means no difference, <0 means the pathologist scores lower, >0
means the pathologist scores higher. `p.value` shows if the slope is signicantly
different from 0.

`p.value` shows you the significance of the difference between the pathologist
and the clinical report in either positive or negative direction. `NA` values for
`p.value` indicate that the model did not converge for some reason, need to
investigate.

This complements the Kappa score and gives you a sense for the direction and
magnitude of disagreement between the pathologist and the clinical report.

```{r}
dt_default_table(
  mods_df
)
```


```{r}
p <- mods_df %>%
  mutate(across(marker, \(x) fct_reorder(x, estimate))) %>%
  ggplot(aes(x = estimate, y = marker, fill = p.value < .05)) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c(`FALSE` = "grey", `TRUE` = "red")) +
  labs(
    y = NULL,
    title = "Difference in mean scores between pathologist and clinical report",
    subtitle = "Positive values mean pathologist scores higher, negative values mean pathologist scores lower"
  )

p
```

## Relating scores with Orion quants

```{r}
orion_vs_scores_all <- power_inner_join(
  scores_all,
  orion_quants,
  by = c("LSP ID", "marker"),
  check = check_specs(
    unmatched_keys_left = "warn",
    unmatched_keys_right = "warn",
    duplicate_keys_right = "warn"
  )
) %>%
  group_by(`LSP ID`) %>%
  mutate(
    both_sources = n_distinct(source) == 2,
    sample_in = if_else(both_sources, "both", "single")
  ) %>%
  ungroup() %>%
  clean_names()
```

### Plots

```{r, fig.width = 16, fig.height = 8, dpi = 200}
p <- orion_vs_scores_all %>%
  drop_na(score) %>%
  ggplot(
    aes(
      x = score,
      y = log_expression,
      color = sample_in
    )
  ) +
  geom_quasirandom(
    orientation = "x",
    shape = 16,
    width = .3
  ) +
  scale_color_manual(
    values = c(
      both = alpha("black", .7),
      single = alpha("black", .1)
    )
  ) +
  ggh4x::facet_grid2(vars(source), vars(marker), scales = "free_y", independent = "y") +
  labs(
    x = "Score",
    y = "log10(expression)",
    color = "Sample in"
  )
p
```


### `cocor`

`cocor` is a package that can be used to compare correlation coefficients between
two groups. Here we are using it to compare which one compares stronger with the
Orion quants: the pathologist's scores or the scores from the clinical report.

We use Kendall correlation here, since the scores are ordinal and we have lots
of ties in the data.

```{r}
orion_vs_scores_all_cor_raw <- orion_vs_scores_all %>%
  filter(both_sources) %>%
  group_by(source, marker) %>%
  summarize(
    res = list(
      cor.test(
        log_expression,
        as.integer(score),
        method = "kendall"
      ) %>%
        tidy() %>%
        bind_cols(
          DescTools::KendallTauB(
            log_expression,
            as.integer(score),
            conf.level = .95
          ) %>%
            as.list() %>%
            as_tibble()
        )
    ),
    .groups = "drop"
  ) %>%
  unnest(res)

orion_vs_scores_cross_cor <- orion_vs_scores_all %>%
  filter(marker == "IgG", both_sources) %>%
  transmute(
    lsp_id, source,
    score = as.integer(score)
  ) %>%
  pivot_wider(names_from = "source", values_from = "score") %>% {
    cor.test(
      ~ pathologist + clinical,
      data = .,
      method = "kendall"
    )
  } %>%
  tidy()

orion_vs_scores_all_cocor <- orion_vs_scores_all_cor_raw %>%
  select(-c(method, alternative)) %>%
  pivot_wider(names_from = "source", values_from = -c(marker)) %>%
  mutate(
    res = map2(
      estimate_clinical, estimate_pathologist,
      \(x, y) cocor::cocor.dep.groups.overlap(
        r.jk = x,
        r.jh = y,
        r.kh = orion_vs_scores_cross_cor$estimate,
        n = 53
      )
    ),
    res_df = map(
      res,
      \(x) as_tibble(x@olkin1967)
    )
  )

orion_vs_scores_all_cocor_df <- orion_vs_scores_all_cocor %>%
  select(-res) %>%
  unnest(res_df)
```

#### `cocor` table

```{r}
orion_vs_scores_all_cocor_df %>%
  select(
    marker,
    starts_with(
      fixed(c("tau_b_", "p.value_", "lwr.ci", "upr.ci"))
    ),
    p.value
  ) %>%
  dt_default_table()
```

Comparing correlations between the pathologist and the clinical report with the
Orion quants. In this table with have the individual correlation coefficients
(`tau_b_clinical` and `tau_b_pahtologist`) and their 95% confidence intervals
(`lwr.ci` and `upr.ci`), as well as p-values for the individual correlations
(`p.value_clinical` and `p.value_pathologist`). These tell you if the individual
correlations are significantly different from 0.

The `p.value` column tells you if the two correlation coefficients are significantly
different from each other. If the p-value is < 0.05, the correlation coefficients
are significantly different.

We're using the `cocor.dep.groups.overlap` flavor of the test
("Performs a test of significance for the difference between two correlations
based on dependent groups (e.g., the same group). The two correlations are
overlapping, i.e., they have one variable in common."). The test is based on
Olkin's z from https://doi.org/10.3102/00028312007002189.

#### `cocor` plot

```{r fig.width = 5, fig.height = 4, dpi = 200}
p <- orion_vs_scores_all_cor_raw %>%
  ggplot(
    aes(
      x = source, y = estimate
    )
  ) +
  geom_line(
    aes(group = marker)
  ) +
  geom_pointrange(
    aes(
      ymin = lwr.ci, ymax = upr.ci,
      color = source
    )
  ) +
  geom_bracket(
    aes(
      xmin = source_clinical,
      xmax = source_pathologist,
      label = signif(p.value, 2)
    ),
    data = orion_vs_scores_all_cocor_df,
    y.position = .75,
    label.size = 2.5
  ) +
  facet_wrap(~marker) +
  scale_color_discrete(guide = "none") +
  scale_y_continuous(
    expand = expansion(mult = c(.05, .15))
  ) +
  labs(
    x = NULL,
    y = "Kendall's tau correlation"
  )

p
```

On the y-axis with have the correlation coefficients of the scores from the
clinical report and the pathologist with the Orion quants, respectively. The
lines show the 95% confidence intervals for the correlation coefficients. The
brackets show the p-values for the difference between the two correlation
coefficients. If the p-value is < 0.05, the correlation coefficients are
significantly different.

### Bayesian models

In this approach we model the relationship between the Orion quants and the
scores from the pathologist and the clinical report. We use a Bayesian ordinal
regression model to do this. The model is a cumulative link mixed model, which
is a type of ordinal regression model that can account for the ordinal nature
of the scores.

We fit a separate model for each marker and score source. We then use the
leave-one-out cross-validation method to compare the models and see which one
performs better, i.e. whether the Orion quants are better at predicting the
pathologist's scores or the clinical report's scores.

```{r}
orion_vs_scores_model_input <- orion_vs_scores_all %>%
  filter(both_sources) %>%
  group_nest(source, marker)

if (!file.exists(here("orion_vs_scores_model_loos.qs"))) {
  mod1 <- brm(
    score ~ log_expression + (1 | lsp_id),
    data = orion_vs_scores_model_input$data[[1]],
    family = cumulative(link = "logit", threshold = "flexible"),
    prior = c(set_prior(prior = "normal(0,10)", class = "Intercept"),
              set_prior(prior = "normal(0,10)", class = "b"),
              set_prior(prior = "cauchy(0,5)", class = "sd")),
    iter = 4000,
    control = list(max_treedepth = 15, adapt_delta = 0.99),
    cores = 4,
    seed = 42,
    save_pars = save_pars(all = TRUE)
  )

  orion_vs_scores_models <- orion_vs_scores_model_input %>%
    rowwise() %>%
    mutate(
      mod = list(
        update(
          mod1,
          newdata = data,
          iter = 4000,
          control = list(max_treedepth = 15, adapt_delta = 0.99),
          cores = 4,
          seed = 42,
          save_pars = save_pars(all = TRUE)
        )
      )
    ) %>%
    ungroup()

  qsave(
    orion_vs_scores_models,
    here("orion_vs_scores_models.qs")
  )

  orion_vs_scores_model_loos <- orion_vs_scores_models %>%
    mutate(
      across(mod, \(x) map(x, \(y) add_criterion(y, "loo", moment_match = TRUE)))
    )

  qsave(
    orion_vs_scores_model_loos,
    here("orion_vs_scores_model_loos.qs")
  )
} else {
  orion_vs_scores_model_loos <- qread(here("orion_vs_scores_model_loos.qs"))
}

orion_vs_scores_model_loo_compare_raw <- orion_vs_scores_model_loos %>%
  select(-data) %>%
  pivot_wider(
    names_from = source, values_from = mod
  ) %>%
  mutate(
    comp = map2(
      pathologist, clinical,
      \(x, y) loo_compare(
        x, y,
        criterion = "loo",
        model_names = c("pathologist", "clinical")
      )
    )
  ) %>%
  select(-c(clinical, pathologist))

orion_vs_scores_model_loo_compare <- orion_vs_scores_model_loo_compare_raw %>%
  mutate(
    across(comp, \(x) map(x, \(y) as_tibble(y, rownames = "model")))
  ) %>%
  unnest(comp)

```


```{r}
post_draws <- orion_vs_scores_model_loos %>%
  rowwise() %>%
  mutate(
    draws = list(
      epred_draws(
        mod,
        newdata = tibble(
          log_expression = seq(min(data$log_expression), max(data$log_expression), length.out = 10)
        ),
        re_formula = NA
      )
    )
  ) %>%
  ungroup() %>%
  select(-c(data, mod)) %>%
  unnest(draws)
```

#### Plot Orion quant as predictor

```{r, fig.width = 14, fig.height = 6, dpi = 200}
p <- post_draws %>%
  mutate(across(.category, ordered)) %>%
  ggplot(
    aes(
      x = log_expression,
      y = .epred,
      color = .category
    )
  ) +
  stat_lineribbon(
    aes(
      fill = .category
    ),
    .width = c(.95),
    alpha = .25
  ) +
  facet_grid(
    source ~ marker,
    scales = "free_x"
  ) +
  labs(
    x = "log10(expression)",
    y = "Probability of score",
    title = "Predicted scores from Orion quants"
  )

p
```

The Bayesian model allows us to compute the probability of each score given some
Orion quantification results. This plot shows the predicted probabilities of
each clinical and pathologist score given the Orion quantification results.

### Bayesian model with score as predictor

```{r}
if (!file.exists(here("orion_vs_scores_reverse_model_loos.qs"))) {
  mod1 <- brm(
    log_expression ~ mo(score) + (1 | lsp_id),
    data = orion_vs_scores_model_input$data[[1]],
    prior = c(set_prior(prior = "normal(0,5)", class = "Intercept"),
              set_prior(prior = "normal(0,5)", class = "b"),
              set_prior(prior = "cauchy(0,5)", class = "sd")),
    iter = 6000,
    control = list(max_treedepth = 15, adapt_delta = 0.99),
    cores = 4,
    seed = 42,
    save_pars = save_pars(all = TRUE)
  )

  orion_vs_scores_reverse_models <- orion_vs_scores_model_input %>%
    rowwise() %>%
    mutate(
      mod = list(
        update(
          mod1,
          newdata = data,
          iter = 6000,
          control = list(max_treedepth = 15, adapt_delta = 0.99),
          cores = 4,
          seed = 42,
          save_pars = save_pars(all = TRUE)
        )
      )
    ) %>%
    ungroup()

  qsave(
    orion_vs_scores_reverse_models,
    here("orion_vs_scores_reverse_models.qs")
  )

  orion_vs_scores_reverse_model_loos <- orion_vs_scores_reverse_models %>%
    mutate(
      across(mod, \(x) map(x, \(y) add_criterion(y, "loo", moment_match = TRUE)))
    )

  qsave(
    orion_vs_scores_reverse_model_loos,
    here("orion_vs_scores_reverse_model_loos.qs")
  )
} else {
  orion_vs_scores_reverse_model_loos <- qread(here("orion_vs_scores_reverse_model_loos.qs"))
}

orion_vs_scores_reverse_model_loo_compare_raw <- orion_vs_scores_reverse_model_loos %>%
  select(-data) %>%
  pivot_wider(
    names_from = source, values_from = mod
  ) %>%
  mutate(
    comp = map2(
      pathologist, clinical,
      \(x, y) loo_compare(
        x, y,
        criterion = "loo",
        model_names = c("pathologist", "clinical")
      )
    )
  ) %>%
  select(-c(clinical, pathologist))

orion_vs_scores_reverse_model_loo_compare <- orion_vs_scores_reverse_model_loo_compare_raw %>%
  mutate(
    across(comp, \(x) map(x, \(y) as_tibble(y, rownames = "model")))
  ) %>%
  unnest(comp)


```


```{r}
post_draws_reverse <- orion_vs_scores_reverse_model_loos %>%
  rowwise() %>%
  mutate(
    draws = list(
      epred_draws(
        mod,
        newdata = distinct(data, score = fct_drop(score)),
        re_formula = NA
      ) %>%
        mutate(
          score = factor(as.character(score), levels = score_order, ordered = TRUE)
        )
    )
  ) %>%
  ungroup() %>%
  select(-c(data, mod)) %>%
  unnest(draws)
```

#### Plot score as predictor

```{r, fig.width = 14, fig.height = 6, dpi = 200}
p <- post_draws_reverse %>%
  # mutate(across(.category, ordered)) %>%
  ggplot(
    aes(
      x = score,
      y = .epred
    )
  ) +
  geom_quasirandom(
    aes(
      y = log_expression,
      color = sample_in
    ),
    orientation = "x",
    shape = 16,
    width = .3,
    data = orion_vs_scores_all %>%
      drop_na(score)
  ) +
  # geom_violin(fill = NA) +
  stat_lineribbon(
    .width = c(.5, .95),
    alpha = .5
  ) +
  scale_fill_brewer() +
  scale_color_manual(
    values = c(
      both = alpha("black", .7),
      single = alpha("black", .1)
    )
  ) +
  ggh4x::facet_grid2(vars(source), vars(marker), scales = "free_y", independent = "y") +
  # stat_lineribbon(
  #   aes(
  #     fill = .category
  #   ),
  #   .width = c(.95),
  #   alpha = .25
  # ) +
  # facet_grid(
  #   source ~ marker,
  #   scales = "free_x"
  # ) +
  labs(
    x = "Score",
    y = "log10(expression)",
    color = "Sample in",
    fill = "Model\nconfidence interval"
  )

p
```
